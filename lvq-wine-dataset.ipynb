{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2000151,"sourceType":"datasetVersion","datasetId":1196587}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from random import seed, randrange\nfrom math import sqrt\nfrom sklearn.datasets import load_wine\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, f1_score","metadata":{"execution":{"iopub.status.busy":"2024-10-10T05:19:50.558991Z","iopub.execute_input":"2024-10-10T05:19:50.559489Z","iopub.status.idle":"2024-10-10T05:19:50.566027Z","shell.execute_reply.started":"2024-10-10T05:19:50.559442Z","shell.execute_reply":"2024-10-10T05:19:50.564511Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def load_wine_dataset():\n    data = load_wine()\n    dataset = []\n    for i in range(len(data.data)):\n        row = list(data.data[i]) + [data.target[i]]\n        dataset.append(row)\n    return dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-10T05:19:50.567957Z","iopub.execute_input":"2024-10-10T05:19:50.568506Z","iopub.status.idle":"2024-10-10T05:19:50.579407Z","shell.execute_reply.started":"2024-10-10T05:19:50.568457Z","shell.execute_reply":"2024-10-10T05:19:50.578129Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Split a dataset into k folds\ndef cross_validation_split(dataset, n_folds):\n    dataset_split = []\n    dataset_copy = list(dataset)\n    fold_size = int(len(dataset) / n_folds)\n    for i in range(n_folds):\n        fold = []\n        while len(fold) < fold_size:\n            index = randrange(len(dataset_copy))\n            fold.append(dataset_copy.pop(index))\n        dataset_split.append(fold)\n    return dataset_split","metadata":{"execution":{"iopub.status.busy":"2024-10-10T05:19:50.580739Z","iopub.execute_input":"2024-10-10T05:19:50.581106Z","iopub.status.idle":"2024-10-10T05:19:50.595358Z","shell.execute_reply.started":"2024-10-10T05:19:50.581066Z","shell.execute_reply":"2024-10-10T05:19:50.594277Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n    folds = cross_validation_split(dataset, n_folds)\n    accuracy_scores = []\n    f1_scores = []\n    for fold in folds:\n        train_set = list(folds)\n        train_set.remove(fold)\n        train_set = sum(train_set, [])\n        test_set = []\n        for row in fold:\n            row_copy = list(row)\n            test_set.append(row_copy)\n            row_copy[-1] = None\n        predicted = algorithm(train_set, test_set, *args)\n        actual = [row[-1] for row in fold]\n        accuracy = accuracy_score(actual, predicted) * 100\n        f1 = f1_score(actual, predicted, average='weighted') * 100\n        accuracy_scores.append(accuracy)\n        f1_scores.append(f1)\n    return accuracy_scores, f1_scores\n\ndef euclidean_distance(row1, row2):\n    distance = 0.0\n    for i in range(len(row1)-1):\n        distance += (row1[i] - row2[i])**2\n    return sqrt(distance)\n\ndef find_bmu(codebooks, test_row):\n    distances = []\n    for codebook in codebooks:\n        dist = euclidean_distance(codebook, test_row)\n        distances.append((codebook, dist))\n    distances.sort(key=lambda tup: tup[1])\n    return distances[0][0]\n\ndef predict(codebooks, test_row):\n    bmu = find_bmu(codebooks, test_row)\n    return bmu[-1]\n\ndef random_codebook(train):\n    n_records = len(train)\n    n_features = len(train[0])\n    codebook = [train[randrange(n_records)][i] for i in range(n_features)]\n    return codebook\n\ndef train_codebooks(train, n_codebooks, lrate, epochs):\n    codebooks = [random_codebook(train) for _ in range(n_codebooks)]\n    for epoch in range(epochs):\n        rate = lrate * (1.0 - (epoch / float(epochs)))\n        for row in train:\n            bmu = find_bmu(codebooks, row)\n            for i in range(len(row) - 1):\n                error = row[i] - bmu[i]\n                if bmu[-1] == row[-1]:\n                    bmu[i] += rate * error\n                else:\n                    bmu[i] -= rate * error\n    return codebooks\n\n# LVQ Algorithm\ndef learning_vector_quantization(train, test, n_codebooks, lrate, epochs):\n    codebooks = train_codebooks(train, n_codebooks, lrate, epochs)\n    predictions = []\n    for row in test:\n        output = predict(codebooks, row)\n        predictions.append(output)\n    return predictions","metadata":{"execution":{"iopub.status.busy":"2024-10-10T05:19:50.597734Z","iopub.execute_input":"2024-10-10T05:19:50.598119Z","iopub.status.idle":"2024-10-10T05:19:50.617991Z","shell.execute_reply.started":"2024-10-10T05:19:50.598081Z","shell.execute_reply":"2024-10-10T05:19:50.616891Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"seed(42)\n\ndataset = load_wine_dataset()\n\nscaler = StandardScaler()\nfor i in range(len(dataset[0])-1):  # Standardize features (ignoring the label)\n    col = [row[i] for row in dataset]\n    scaled_col = scaler.fit_transform([[x] for x in col])\n    for j in range(len(dataset)):\n        dataset[j][i] = scaled_col[j][0]\n\nn_folds = 5\nlearn_rate = 0.3\nn_epochs = 50\nn_codebooks = 20\n\naccuracy_scores, f1_scores = evaluate_algorithm(dataset, learning_vector_quantization, n_folds, n_codebooks, learn_rate, n_epochs)\nprint('Accuracy Scores: %s' % accuracy_scores)\nprint('Mean Accuracy: %.3f%%' % (sum(accuracy_scores)/float(len(accuracy_scores))))\nprint('F1 Scores: %s' % f1_scores)\nprint('Mean F1 Score: %.3f' % (sum(f1_scores)/float(len(f1_scores))))","metadata":{"execution":{"iopub.status.busy":"2024-10-10T05:19:50.619587Z","iopub.execute_input":"2024-10-10T05:19:50.620050Z","iopub.status.idle":"2024-10-10T05:19:57.284692Z","shell.execute_reply.started":"2024-10-10T05:19:50.619995Z","shell.execute_reply":"2024-10-10T05:19:57.283622Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Accuracy Scores: [97.14285714285714, 100.0, 91.42857142857143, 91.42857142857143, 97.14285714285714]\nMean Accuracy: 95.429%\nF1 Scores: [97.10808461315561, 100.0, 91.3647937935794, 91.3886113886114, 97.13497536945813]\nMean F1 Score: 95.399\n","output_type":"stream"}]},{"cell_type":"markdown","source":"En esta actividad, se implementan varias mejoras clave con respecto al código original de LVQ. Se cambió del conjunto de datos, escogiendo el dataset de vino de la biblioteca `sklearn.datasets`. El motivo de escoger este dataset proviene de la recomendación de buscar datasets de clasificación en repositorios como el de aprendizaje automático de UCI. Adicionalmente use`StandardScaler` para estandarizar las características antes de entrenar los modelos. El código original solo calculaba la precisión, pero en el mío se incluye el cálculo del F1 para mejor comprensión de las predicciones. Se mantuvieron la mayoría de las funciones clave, como la de encontrar la unidad de mejor coincidencia, entrenar los codebooks y realizar predicciones. A diferencia del notebook de SOM, decidí no explorar hiperparámetros para este problema debido a los buenos resultados obtenidos a través de la prueba y error. Aprendi que así como fue el caso para SOM, los dos se basan en encontrar el Best Matching Unit para optimizar las predicciones de los modelos. Este enfoque para resolver problemas de agrupación y de clasificaciones me brindaron un nuevo panorama de las alternativas que hay entre las redes neuronales densas y los metodos de aprendizaje automatico tradicionales. Ambos metodos brindan un nuevo lado por donde explorar para resolver este tipo de problemas.","metadata":{}}]}